{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b292402-fc1d-414b-b5d4-77b7f34ff2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculates MFIS estimates for failure probability using ECL\n",
    "adaptive designs from the Hartmann6 experiment and space-filling designs.\n",
    "\n",
    "@author: D. Austin Cole  austin.cole8@vt.edu\n",
    "\"\"\"\n",
    "\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyDOE import lhs\n",
    "import scipy.stats as ss\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "import os\n",
    "import sys\n",
    "fileDir = os.getcwd()\n",
    "sourcePath = os.path.join(fileDir, 'MFISPy')\n",
    "sys.path.append(sourcePath)\n",
    "from mfis import BiasingDistribution\n",
    "from mfis import MultiFidelityIS\n",
    "from mfis.mv_independent_distribution import MVIndependentDistribution\n",
    "\n",
    "sourcePath = fileDir\n",
    "sys.path.append(sourcePath)\n",
    "from eclGP import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import dill\n",
    "from psis import psislw\n",
    "import glob\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3faaf138-6901-4099-8125-c86ef7d379f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/Ex_1\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb33a64-560b-442a-ac88-7c969aa9943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Start_time = time.time()\n",
    "today = date.today()\n",
    "case = 'Ex1'\n",
    "Model = Numerical_Ex1()\n",
    "bounds = ((-5,5),(-5,5),(-5,5),(-5,5))\n",
    "threshold = 18.99\n",
    "dim = len(bounds)\n",
    "alpha = 0.01\n",
    "\n",
    "N_HIGH_FIDELITY_INPUTS = 1200\n",
    "n_init = 10*dim\n",
    "n_select = 600 - n_init\n",
    "MC_REPS = 25\n",
    "batch_size = 10\n",
    "\n",
    "mfis_probs = np.ones((MC_REPS, 3))\n",
    "#gauss_kernel = 1.0 * RBF(length_scale= np.repeat(0.5, dim),length_scale_bounds=(1e-4, 1e4))\n",
    "# Stochastic\n",
    "gauss_kernel = 1.0 * RBF(length_scale= np.repeat(0.5, dim),length_scale_bounds=(1e-4, 1e4)) + WhiteKernel(noise_level=1e-2, noise_level_bounds=(1e-10, 1e1))\n",
    "\n",
    "\n",
    "###############################\n",
    "# 0. Build Initial design\n",
    "###############################\n",
    "\n",
    "stat_results = np.zeros((MC_REPS,dim+1))\n",
    "initial_designs = np.zeros((n_init, (dim+1)*MC_REPS))\n",
    "today = date.today()\n",
    "file_date = today.strftime(\"%m%d%y\")\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "for i in range(MC_REPS):\n",
    "    X0 = lhs(dim, n_init)\n",
    "    X0 = ss.norm.ppf(X0)\n",
    "    Y0 = Model.predict(X0)\n",
    "\n",
    "    initial_designs[:,((dim+1)*i):((dim+1)*(i+1))] = \\\n",
    "        np.hstack((X0, Y0.reshape((-1,1))))\n",
    "\n",
    "# print(X0)\n",
    "# print(Y0)\n",
    "# np.savetxt(f'data/Initial_designs_{case}_{file_date}_2.csv',initial_designs, delimiter = \",\")\n",
    "np.savetxt(f'data/Ex_1/Initial_designs_{case}_{file_date}.csv',initial_designs, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860502f4-37e8-4d67-ae72-7260c0202a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified upper bound 10.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "100%|██████████| 560/560 [08:31<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# 1. Build ECL GP\n",
    "###############################\n",
    "initial_designs_df = pd.read_csv(f\"data/Ex_1/Initial_designs_{case}_{file_date}.csv\",header=None) # pd.read_csv(f\"data/Initial_designs_{case}_{file_date}_2.csv\",header=None)\n",
    "initial_designs = np.array(initial_designs_df)\n",
    "ecl_designs = np.zeros((n_init+n_select, (dim+1)*MC_REPS))\n",
    "ecl_times = np.zeros((MC_REPS,))\n",
    "ecl_batch_designs = np.zeros((n_init+n_select, (dim+1)*MC_REPS))\n",
    "ecl_batch_times = np.zeros((MC_REPS,))\n",
    "\n",
    "def limit_state_function(y):\n",
    "    return y - threshold\n",
    "\n",
    "for i in range(MC_REPS): \n",
    "    X0 = initial_designs[:,((dim+1)*i):((dim+1)*(i+1)-1)]\n",
    "    Y0 = initial_designs[:,(dim+1)*(i+1)-1]\n",
    "      \n",
    "    ## Adaptive design with ECL\n",
    "    ecl_start_time = time.time()\n",
    "    init_gp = GPR(kernel=gauss_kernel, alpha=1e-6)\n",
    "    init_gp.fit(X0, Y0)\n",
    "    eclgp = EntropyContourLocatorGP(init_gp, limit_state_function)\n",
    "    eclgp.fit(n_select, Model, bounds)\n",
    "\n",
    "    ecl_designs[:,((dim+1)*i):((dim+1)*(i+1))] = \\\n",
    "        np.hstack((eclgp.X_, eclgp.y_.reshape((-1,1))))\n",
    "    ecl_times[i] = (time.time() - ecl_start_time)/60\n",
    "    \n",
    "#     ## Adaptive design with ECL.batch\n",
    "#     ecl_batch_start_time = time.time()\n",
    "#     eclgp_batch = EntropyContourLocatorGP(init_gp,limit_state_function)\n",
    "    \n",
    "#     eclgp_batch.fit(np.int(n_select/batch_size), Model, bounds, batch_size=batch_size)    \n",
    "#     ecl_batch_designs[:,((dim+1)*i):((dim+1)*(i+1))] = \\\n",
    "#         np.hstack((eclgp_batch.X_, eclgp_batch.y_.reshape((-1,1))))\n",
    "#     ecl_batch_times[i] = (time.time() - ecl_batch_start_time)/60\n",
    "\n",
    "    print(i)\n",
    "    np.savetxt(f'data/Ex_1/ecl_designs_{case}_{file_date}.csv', # data/ecl_designs_{case}_{file_date}_2.csv\n",
    "                ecl_designs, delimiter = \",\")\n",
    "    np.savetxt(f'data/Ex_1/ecl_designs_times_{case}_{file_date}.csv', # data/ecl_designs_times_{case}_{file_date}_2.csv\n",
    "            ecl_times, delimiter = \",\")\n",
    "\n",
    "#     np.savetxt(f'data/ecl_batch10_designs_{case}_{file_date}.csv',\n",
    "#                 ecl_batch_designs, delimiter = \",\")\n",
    "#     np.savetxt(f'data/ecl_batch10_times_{case}_{file_date}.csv', \n",
    "#                 ecl_batch_times, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd5444c0-a9b2-44c4-b333-a8e390cc6303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 1 of parameter k1__k2__length_scale is close to the specified upper bound 10000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 2 of parameter k1__k2__length_scale is close to the specified upper bound 10000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified upper bound 10.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0 th Experiment Total Elapsed time:  7.2707109451293945\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# 2. MFIS\n",
    "####################################\n",
    "unif_dist = ss.uniform(0,1)\n",
    "m1 = 0\n",
    "s1 = 1\n",
    "norm_dist = ss.norm(loc=m1, scale=s1)\n",
    "\n",
    "input_distribution = MVIndependentDistribution(\n",
    "    distributions=[norm_dist, norm_dist, norm_dist, norm_dist])\n",
    "\n",
    "## Estimate failure probability (alpha)\n",
    "'''\n",
    "num_failures = np.zeros((100,))\n",
    "for i in range(len(num_failures)):\n",
    "    new_X = input_distribution.draw_samples(1000000)\n",
    "    new_Y = hartmann_model.predict(new_X)\n",
    "    num_failures[i] = np.sum(hartmann_limit_state_function(new_Y)>0)\n",
    "    print(num_failures[i])\n",
    "    print(i)\n",
    "alpha = np.mean(num_failures)/len(new_X)\n",
    "'''\n",
    "mfis_probs = np.ones((MC_REPS, 3))\n",
    "\n",
    "ecl_designs_df = pd.read_csv(f\"data/Ex_1/ecl_designs_{case}_{file_date}.csv\",header=None) #pd.read_csv(f\"data/ecl_designs_{case}_{file_date}.csv\",header=None)\n",
    "ecl_designs = np.array(ecl_designs_df)\n",
    "\n",
    "\n",
    "weight_is_all_sample_list = []\n",
    "weight_ucb_all_sample_list = []\n",
    "    \n",
    "for j in range(MC_REPS): \n",
    "    start_time = time.time()\n",
    "    ecl_gp = None\n",
    "       \n",
    "    ## ECL\n",
    "    design_X = ecl_designs[:, ((dim+1)*j):((dim+1)*(j+1)-1)]\n",
    "    design_Y = ecl_designs[:, (dim+1)*(j+1)-1]\n",
    "    ecl_gp =  GPR(kernel=gauss_kernel, alpha=1e-6)\n",
    "    ecl_gp.fit(design_X, design_Y)\n",
    "    \n",
    "    ## ECL-Batch\n",
    "#     design_X_batch = ecl_designs_batch[:, ((dim+1)*j):((dim+1)*(j+1)-1)]\n",
    "#     design_Y_batch = ecl_designs_batch[:, (dim+1)*(j+1)-1]\n",
    "#     ecl_gp_batch =  GPR(kernel=gauss_kernel, alpha=1e-6)\n",
    "#     ecl_gp_batch.fit(design_X_batch, design_Y_batch)\n",
    "            \n",
    "  \n",
    "    # Initialize Biasing Distributions\n",
    "    ecl_bd =  BiasingDistribution(trained_surrogate=ecl_gp,\n",
    "                            limit_state=limit_state_function,\n",
    "                            input_distribution=input_distribution)\n",
    "    ecl_bd_ucb =  BiasingDistribution(trained_surrogate=ecl_gp,\n",
    "                            limit_state=limit_state_function,\n",
    "                            input_distribution=input_distribution)\n",
    "    \n",
    "    ## Fit Biasing Distributions\n",
    "    ecl_failed_inputs = np.empty((0, dim))   \n",
    "    ecl_failed_inputs_ucb = np.empty((0, dim))\n",
    "\n",
    "    # Get sample outputs from GPs and classify failures  \n",
    "    for k in range(100):\n",
    "        sample_inputs = input_distribution.draw_samples(10000) \n",
    "        \n",
    "        ecl_sample_outputs, ecl_sample_std = \\\n",
    "            ecl_gp.predict(sample_inputs, return_std=True)\n",
    "        ecl_failed_inputs_new = sample_inputs[\n",
    "            limit_state_function(ecl_sample_outputs.flatten()) > 0,:]\n",
    "        ecl_failed_inputs = np.vstack((ecl_failed_inputs,\n",
    "                                       ecl_failed_inputs_new))\n",
    "\n",
    "        ecl_failed_inputs_ucb_new = sample_inputs[\n",
    "            limit_state_function(\n",
    "                ecl_sample_outputs.flatten() + 1.645*ecl_sample_std) > 0,:]\n",
    "        ecl_failed_inputs_ucb = np.vstack((ecl_failed_inputs_ucb,\n",
    "                                           ecl_failed_inputs_ucb_new))\n",
    "        \n",
    "        if (k % 100) == 0:\n",
    "            print(k)\n",
    "\n",
    "    if len(ecl_failed_inputs) < 1:\n",
    "        mfis_probs[j, 0] = 0\n",
    "    else:\n",
    "        ecl_bd.fit_from_failed_inputs(ecl_failed_inputs,\n",
    "                                   max_clusters=10, covariance_type='diag')\n",
    "        ## Failure probability estimates\n",
    "        XX_ecl = ecl_bd.draw_samples(N_HIGH_FIDELITY_INPUTS - n_init - n_select)\n",
    "        hf_ecl_outputs = Model.predict(XX_ecl)\n",
    "        multi_IS_ecl = \\\n",
    "            MultiFidelityIS(limit_state=limit_state_function,\n",
    "                            biasing_distribution=ecl_bd,\n",
    "                            input_distribution=input_distribution,\n",
    "                            bounds=bounds)\n",
    "        ecl_mfis_stats = multi_IS_ecl.get_failure_prob_estimate(XX_ecl,\n",
    "                                                              hf_ecl_outputs)\n",
    "        weights_is = multi_IS_ecl.calc_importance_weights(XX_ecl)\n",
    "        weight_is_all_sample_list.append(weights_is)\n",
    "        mfis_probs[j, 0] = ecl_mfis_stats[0]\n",
    "        \n",
    "\n",
    "    print(j)\n",
    "    # End timer\n",
    "    end_time = time.time()\n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(j,'th Experiment',\"Total Elapsed time: \", elapsed_time) \n",
    "    \n",
    "    mfis_probs_df = pd.DataFrame(mfis_probs)\n",
    "    mfis_probs_df = mfis_probs_df.rename(columns={0:'ECL'})\n",
    "    mfis_probs_df.to_csv(f'data/Ex_1/{case}_mfis_estimates_Experiment_{file_date}_{MC_REPS}.csv',index=False)\n",
    "dill.dump_session(f'{case}_Stochastic_num_{N_HIGH_FIDELITY_INPUTS}_{MC_REPS}rep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388210f1-4aaf-475d-a63f-6cd458d342dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
